"""
ALTERNATIVE NMS APPROACHES FOR MAXIMUM ACCURACY

Option 1: Hybrid approach - ONNX for inference, CPU NMS for postprocessing
Option 2: TensorRT EfficientNMS plugin (NVIDIA GPUs only)
Option 3: ONNX built-in NonMaxSuppression operator
"""

import torch
import torch.nn as nn
import numpy as np
import yaml
import json
import os
from tqdm import tqdm
import onnxruntime as ort
from torchvision.ops import nms, batched_nms
from pycocotools.coco import COCO
from pycocotools.cocoeval import COCOeval
from utils.utils import preprocess, invert_affine, boolean_string
from backbone import EfficientDetBackbone
from efficientdet.utils import BBoxTransform, ClipBoxes

# =============================================================================
# OPTION 1: HYBRID APPROACH - ONNX INFERENCE + CPU NMS (RECOMMENDED)
# =============================================================================

class EfficientDetONNXOnly(nn.Module):
    """
    Export only the backbone + postprocessing to ONNX, do NMS on CPU
    This gives the best accuracy since we use the exact same NMS as PyTorch
    """
    def __init__(self, backbone, score_threshold=0.05):
        super().__init__()
        self.backbone = backbone
        self.score_threshold = score_threshold
        self.regressBoxes = BBoxTransform()
        self.clipBoxes = ClipBoxes()
    
    def forward(self, x):
        features, regression, classification, anchors = self.backbone(x)
        
        # Transform boxes
        transformed_anchors = self.regressBoxes(anchors, regression)
        transformed_anchors = self.clipBoxes(transformed_anchors, x)
        
        # Return raw outputs for CPU postprocessing
        # Classification already has sigmoid applied in Classifier.forward()
        return transformed_anchors, classification, anchors

class HybridEfficientDetEvaluator:
    """
    Uses ONNX for inference, CPU for NMS postprocessing
    This approach typically gives the best accuracy
    """
    def __init__(self, model_path, score_threshold=0.05, nms_threshold=0.5, max_detections=100, device='cpu'):
        self.score_threshold = score_threshold
        self.nms_threshold = nms_threshold
        self.max_detections = max_detections
        
        # Setup ONNX Runtime
        providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if device == 'cuda' else ['CPUExecutionProvider']
        self.session = ort.InferenceSession(model_path, providers=providers)
        
        self.input_name = self.session.get_inputs()[0].name
        self.output_names = [output.name for output in self.session.get_outputs()]
        
        print(f"Hybrid model loaded. Outputs: {self.output_names}")
    
    def predict(self, image_tensor):
        """Run ONNX inference then CPU NMS"""
        if hasattr(image_tensor, 'numpy'):
            image_np = image_tensor.numpy()
        else:
            image_np = image_tensor
            
        # Get raw outputs from ONNX
        outputs = self.session.run(self.output_names, {self.input_name: image_np})
        transformed_anchors, classification, anchors = outputs
        
        # Convert back to torch tensors for NMS
        transformed_anchors = torch.from_numpy(transformed_anchors)
        classification = torch.from_numpy(classification)
        
        # Apply the same postprocessing as original PyTorch model
        return self._cpu_postprocess(transformed_anchors, classification)
    
    def _cpu_postprocess(self, transformed_anchors, classification):
        """Apply CPU-based NMS identical to original PyTorch implementation"""
        # Get max scores and classes
        scores = torch.max(classification, dim=2, keepdim=True)[0]
        scores_over_thresh = (scores > self.score_threshold)[:, :, 0]
        
        batch_results = []
        
        for i in range(transformed_anchors.shape[0]):
            if scores_over_thresh[i].sum() == 0:
                batch_results.append({
                    'rois': np.array([]), 
                    'class_ids': np.array([]), 
                    'scores': np.array([])
                })
                continue

            classification_per = classification[i, scores_over_thresh[i], ...].permute(1, 0)
            transformed_anchors_per = transformed_anchors[i, scores_over_thresh[i], ...]
            scores_per = scores[i, scores_over_thresh[i], ...]
            scores_classes = classification_per.max(dim=0)
            
            # Use the EXACT same NMS as original PyTorch model
            anchors_nms_idx = batched_nms(
                transformed_anchors_per, 
                scores_per[:, 0], 
                scores_classes[1], 
                iou_threshold=self.nms_threshold
            )

            if anchors_nms_idx.shape[0] != 0:
                classes = scores_classes[1][anchors_nms_idx]
                scores_ = scores_per[anchors_nms_idx]
                boxes = transformed_anchors_per[anchors_nms_idx, :]
                
                # Limit detections
                if len(boxes) > self.max_detections:
                    boxes = boxes[:self.max_detections]
                    scores_ = scores_[:self.max_detections]
                    classes = classes[:self.max_detections]
                
                batch_results.append({
                    'rois': boxes.cpu().numpy(), 
                    'class_ids': classes.cpu().numpy(), 
                    'scores': scores_.cpu().numpy()
                })
            else:
                batch_results.append({
                    'rois': np.array([]), 
                    'class_ids': np.array([]), 
                    'scores': np.array([])
                })
        
        # Return first batch result (assuming batch_size=1)
        result = batch_results[0]
        return result['rois'], result['scores'][:, 0], result['class_ids']

def convert_to_hybrid_onnx(project_file, weights_path, output_path, compound_coef=2, score_threshold=0.05):
    """Convert to hybrid ONNX model (inference only, NMS on CPU)"""
    device = torch.device('cpu')
    
    class Params:
        def __init__(self, project_file):
            self.params = yaml.safe_load(open(project_file).read())
        def __getattr__(self, item):
            return self.params.get(item, None)
    
    params = Params(project_file)
    input_sizes = [512, 640, 768, 896, 1024, 1280, 1280, 1536, 1536]
    input_size = input_sizes[compound_coef]
    
    # Create backbone model
    backbone = EfficientDetBackbone(
        num_classes=len(params.obj_list), 
        compound_coef=compound_coef, 
        onnx_export=True,
        ratios=eval(params.anchors_ratios), 
        scales=eval(params.anchors_scales)
    ).to(device)
    
    backbone.backbone_net.model.set_swish(memory_efficient=False)
    backbone.load_state_dict(torch.load(weights_path, map_location=device))
    
    # Create hybrid model (no NMS in ONNX)
    model = EfficientDetONNXOnly(backbone, score_threshold)
    model.eval()
    
    dummy_input = torch.randn((1, 3, input_size, input_size), dtype=torch.float32).to(device)
    
    print("Converting to hybrid ONNX (inference only, NMS on CPU)...")
    
    torch.onnx.export(
        model, 
        dummy_input, 
        output_path,
        verbose=False,
        input_names=['input'],
        output_names=['boxes', 'classification', 'anchors'],
        opset_version=11,
        do_constant_folding=True
    )
    
    print(f"Hybrid ONNX model saved to: {output_path}")
    return True

def evaluate_hybrid_onnx(img_path, set_name, image_ids, coco, model, compound_coef, params):
    """Evaluate hybrid ONNX model with CPU NMS"""
    results = []
    input_sizes = [512, 640, 768, 896, 1024, 1280, 1280, 1536, 1536]
    
    for image_id in tqdm(image_ids):
        image_info = coco.loadImgs(image_id)[0]
        image_path = img_path + image_info['file_name']

        try:
            ori_imgs, framed_imgs, framed_metas = preprocess(
                [image_path],
                max_size=input_sizes[compound_coef],
                mean=params['mean'],
                std=params['std']
            )
        except Exception as e:
            continue

        # Prepare input
        x = framed_imgs[0]
        x = np.expand_dims(x, axis=0)
        x = np.transpose(x, (0, 3, 1, 2))

        try:
            # Get predictions with CPU NMS
            boxes, scores, classes = model.predict(x)
            
            if len(boxes) == 0:
                continue
                
        except Exception as e:
            print(f"Inference failed for image {image_id}: {e}")
            continue

        # Apply coordinate transformation
        try:
            preds = [{
                'rois': boxes,
                'scores': scores, 
                'class_ids': classes
            }]
            
            preds = invert_affine(framed_metas, preds)[0]
            
            final_boxes = preds['rois']
            final_scores = preds['scores']
            final_classes = preds['class_ids']
            
        except Exception as e:
            final_boxes = boxes
            final_scores = scores
            final_classes = classes

        # Process results
        for i in range(len(final_boxes)):
            box = final_boxes[i]
            score = float(final_scores[i])
            class_id = int(final_classes[i])
            
            if score <= 0 or score > 1:
                continue
                
            x1, y1, x2, y2 = box
            width = x2 - x1
            height = y2 - y1
            
            if width <= 0 or height <= 0:
                continue
                
            image_result = {
                'image_id': image_id,
                'category_id': class_id + 1,
                'score': score,
                'bbox': [float(x1), float(y1), float(width), float(height)],
            }
            results.append(image_result)

    if not len(results):
        raise Exception("No valid predictions generated")

    # Save results
    filepath = f'{set_name}_bbox_results_hybrid.json'
    if os.path.exists(filepath):
        os.remove(filepath)
    json.dump(results, open(filepath, 'w'), indent=4)
    
    print(f"Generated {len(results)} predictions")
    return filepath

# =============================================================================
# OPTION 2: TENSORRT EFFICIENTNMS (NVIDIA GPUS ONLY)
# =============================================================================

def add_efficientnms_to_onnx(onnx_model_path, output_path, num_classes, 
                            score_threshold=0.05, nms_threshold=0.5, max_detections=100):
    """
    Add TensorRT EfficientNMS plugin to ONNX model
    This requires NVIDIA GPU and TensorRT
    """
    try:
        import onnx
        import onnx_graphsurgeon as gs
        
        # Load the ONNX model
        graph = gs.import_onnx(onnx.load(onnx_model_path))
        
        # Find the output nodes
        boxes_output = None
        scores_output = None
        
        for node in graph.nodes:
            if 'boxes' in node.name.lower():
                boxes_output = node.outputs[0]
            elif 'scores' in node.name.lower() or 'classification' in node.name.lower():
                scores_output = node.outputs[0]
        
        if boxes_output is None or scores_output is None:
            raise ValueError("Could not find boxes and scores outputs in ONNX model")
        
        # Create EfficientNMS node
        efficientnms_node = gs.Node(
            op="EfficientNMS_TRT",
            name="EfficientNMS",
            attrs={
                "background_class": -1,
                "box_coding": 0,
                "iou_threshold": nms_threshold,
                "max_output_boxes": max_detections,
                "plugin_version": "1",
                "score_activation": False,
                "score_threshold": score_threshold,
            }
        )
        
        # Create new outputs
        num_detections = gs.Variable("num_detections", dtype=np.int32, shape=[1, 1])
        nms_boxes = gs.Variable("detection_boxes", dtype=np.float32, shape=[1, max_detections, 4])
        nms_scores = gs.Variable("detection_scores", dtype=np.float32, shape=[1, max_detections])
        nms_classes = gs.Variable("detection_classes", dtype=np.int32, shape=[1, max_detections])
        
        efficientnms_node.inputs = [boxes_output, scores_output]
        efficientnms_node.outputs = [num_detections, nms_boxes, nms_scores, nms_classes]
        
        # Add the node to the graph
        graph.nodes.append(efficientnms_node)
        
        # Set graph outputs
        graph.outputs = [num_detections, nms_boxes, nms_scores, nms_classes]
        
        # Clean up and save
        graph.cleanup()
        onnx.save(gs.export_onnx(graph), output_path)
        
        print(f"EfficientNMS model saved to: {output_path}")
        return True
        
    except ImportError:
        print("❌ TensorRT or onnx-graphsurgeon not available")
        print("Install with: pip install onnx-graphsurgeon")
        return False
    except Exception as e:
        print(f"❌ EfficientNMS conversion failed: {e}")
        return False

# =============================================================================
# USAGE EXAMPLES
# =============================================================================

def main_hybrid_approach():
    """Example usage of hybrid approach (RECOMMENDED)"""
    parser = argparse.ArgumentParser('Hybrid EfficientDet ONNX with CPU NMS')
    parser.add_argument('-p', '--project', type=str, default='projects/abhil.yml')
    parser.add_argument('-c', '--compound_coef', type=int, default=2)
    parser.add_argument('-w', '--weights', type=str, required=True)
    parser.add_argument('-o', '--output', type=str, default='efficientdet_hybrid.onnx')
    parser.add_argument('--score_threshold', type=float, default=0.05)
    parser.add_argument('--nms_threshold', type=float, default=0.5)
    parser.add_argument('--max_detections', type=int, default=100)
    parser.add_argument('--evaluate', action='store_true', help='Run evaluation after conversion')
    
    args = parser.parse_args()
    
    # Convert to hybrid ONNX
    success = convert_to_hybrid_onnx(
        args.project, args.weights, args.output, args.compound_coef, args.score_threshold
    )
    
    if success and args.evaluate:
        # Load project parameters
        params = yaml.safe_load(open(args.project).read())
        
        # Setup evaluation
        SET_NAME = params['val_set']
        VAL_GT = f'datasets/{params["project_name"]}/annotations/instances_{SET_NAME}.json'
        VAL_IMGS = f'datasets/{params["project_name"]}/{SET_NAME}/'
        
        coco_gt = COCO(VAL_GT)
        image_ids = coco_gt.getImgIds()[:1000]  # Limit for testing
        
        # Create hybrid evaluator
        model = HybridEfficientDetEvaluator(
            args.output, args.score_threshold, args.nms_threshold, args.max_detections
        )
        
        # Run evaluation
        results_file = evaluate_hybrid_onnx(
            VAL_IMGS, SET_NAME, image_ids, coco_gt, model, args.compound_coef, params
        )
        
        # Evaluate with COCO metrics
        coco_pred = coco_gt.loadRes(results_file)
        coco_eval = COCOeval(coco_gt, coco_pred, 'bbox')
        coco_eval.params.imgIds = image_ids
        coco_eval.evaluate()
        coco_eval.accumulate()
        coco_eval.summarize()

if __name__ == '__main__':
    import argparse
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == 'hybrid':
        main_hybrid_approach()
    else:
        print("Available approaches:")
        print("1. python script.py hybrid - Use hybrid approach (ONNX + CPU NMS)")
        print("2. Use the proper ONNX NMS from the previous artifact")
        print("3. Use TensorRT EfficientNMS (GPU only)")
        print()
        print("RECOMMENDATION: Use hybrid approach for best accuracy")

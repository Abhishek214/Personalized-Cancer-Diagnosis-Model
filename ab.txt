# Targeted ONNX Conversion Fix Based on Debug Output
# File: convert_onnx_targeted_fix.py

import torch
import torch.nn as nn
import yaml
import argparse
import os
import numpy as np
import warnings
from backbone import EfficientDetBackbone
from efficientdet.utils import BBoxTransform, ClipBoxes
import torchvision

# Suppress warnings
warnings.filterwarnings("ignore")

def replace_batched_nms():
    """Replace batched_nms with ONNX-compatible version"""
    def onnx_batched_nms(boxes, scores, idxs, iou_threshold):
        if boxes.numel() == 0:
            return torch.empty((0,), dtype=torch.int64, device=boxes.device)
        
        max_coordinate = boxes.max()
        offsets = idxs.to(boxes) * (max_coordinate + torch.tensor(1.0))
        boxes_for_nms = boxes + offsets[:, None]
        keep = torchvision.ops.nms(boxes_for_nms, scores, iou_threshold)
        return keep
    
    torchvision.ops.boxes.batched_nms = onnx_batched_nms
    print("✓ Replaced batched_nms for ONNX compatibility")

def fix_swish_for_your_model(model):
    """Fix swish activations specifically for your model structure"""
    print("=== Fixing Swish Activations ===")
    
    # FIX 1: Fix the backbone EfficientNet swish (the main issue)
    try:
        if hasattr(model, 'backbone_net') and hasattr(model.backbone_net, 'model'):
            if hasattr(model.backbone_net.model, 'set_swish'):
                model.backbone_net.model.set_swish(memory_efficient=False)
                print("✓ Fixed backbone EfficientNet swish (set_swish method)")
            else:
                print("⚠ No set_swish method found on backbone")
        
        # FIX 2: Replace MemoryEfficientSwish with regular Swish in all _blocks
        swish_replaced = 0
        for name, module in model.named_modules():
            if '_swish' in name and 'MemoryEfficientSwish' in str(type(module)):
                # Get the parent module and replace the _swish attribute
                parent_name = '.'.join(name.split('.')[:-1])
                if parent_name:
                    parent_module = model
                    for part in parent_name.split('.'):
                        parent_module = getattr(parent_module, part)
                    
                    # Replace with regular Swish
                    from efficientnet.utils import Swish
                    setattr(parent_module, '_swish', Swish())
                    swish_replaced += 1
                    print(f"✓ Replaced MemoryEfficientSwish at {name}")
        
        print(f"✓ Replaced {swish_replaced} MemoryEfficientSwish modules")
        return True
        
    except Exception as e:
        print(f"❌ Swish fixing failed: {e}")
        return False

class YourModelONNXWrapper(nn.Module):
    """ONNX wrapper specifically designed for your model structure"""
    
    def __init__(self, backbone, score_threshold=0.05, nms_threshold=0.5, max_detections=100):
        super().__init__()
        self.backbone = backbone
        self.score_threshold = score_threshold
        self.nms_threshold = nms_threshold
        self.max_detections = max_detections
        
        self.regressBoxes = BBoxTransform()
        self.clipBoxes = ClipBoxes()
    
    def forward(self, x):
        # Get backbone outputs - your model returns (features, regression, classification, anchors)
        features, regression, classification, anchors = self.backbone(x)
        
        # Apply postprocessing exactly like original PyTorch model
        transformed_anchors = self.regressBoxes(anchors, regression)
        transformed_anchors = self.clipBoxes(transformed_anchors, x)
        
        # Get scores and classes
        scores = torch.max(classification, dim=2, keepdim=True)[0]
        classes = torch.max(classification, dim=2)[1]
        
        # Apply score threshold
        score_mask = scores > self.score_threshold
        score_mask = score_mask[:, :, 0]  # Remove last dimension
        
        # Pre-allocate outputs
        batch_size = x.shape[0]
        final_boxes = torch.zeros((batch_size, self.max_detections, 4), device=x.device)
        final_scores = torch.zeros((batch_size, self.max_detections), device=x.device)
        final_classes = torch.zeros((batch_size, self.max_detections), device=x.device, dtype=torch.long)
        
        # Process each image in batch
        for i in range(batch_size):
            if score_mask[i].any():
                valid_boxes = transformed_anchors[i, score_mask[i], :]
                valid_scores = scores[i, score_mask[i], 0]
                valid_classes = classes[i, score_mask[i]]
                
                if len(valid_boxes) > 0:
                    # Apply NMS
                    keep = torchvision.ops.batched_nms(
                        valid_boxes, valid_scores, valid_classes, self.nms_threshold
                    )
                    
                    # Limit detections
                    keep = keep[:self.max_detections]
                    num_keep = len(keep)
                    
                    if num_keep > 0:
                        final_boxes[i, :num_keep] = valid_boxes[keep]
                        final_scores[i, :num_keep] = valid_scores[keep]
                        final_classes[i, :num_keep] = valid_classes[keep]
        
        return final_boxes, final_scores, final_classes

class Params:
    def __init__(self, project_file):
        self.params = yaml.safe_load(open(project_file).read())
    def __getattr__(self, item):
        return self.params.get(item, None)

def test_model_inference_fixed(model, input_size=768):
    """Fixed version of inference test that handles your model's tuple output"""
    print(f"\n=== Fixed Model Inference Test ===")
    
    try:
        model.eval()
        dummy_input = torch.randn(1, 3, input_size, input_size)
        
        print(f"Input shape: {dummy_input.shape}")
        
        with torch.no_grad():
            outputs = model(dummy_input)
            
        print(f"✓ Inference successful!")
        print(f"Number of outputs: {len(outputs)}")
        
        # Handle tuple output correctly
        for i, output in enumerate(outputs):
            if hasattr(output, 'shape'):
                print(f"  Output {i}: {output.shape}, dtype: {output.dtype}")
                if hasattr(output, 'numel'):
                    non_zero = torch.count_nonzero(output)
                    print(f"    Non-zero elements: {non_zero}")
            else:
                print(f"  Output {i}: {type(output)} (not a tensor)")
                
        return True, outputs
        
    except Exception as e:
        print(f"❌ Inference failed: {e}")
        import traceback
        traceback.print_exc()
        return False, None

def convert_your_model_to_onnx(project_file, weights_path, output_path, compound_coef=2, 
                              score_threshold=0.05, nms_threshold=0.5, max_detections=100):
    
    print("=== Converting Your Specific Model to ONNX ===")
    
    # Apply fixes
    replace_batched_nms()
    
    device = torch.device('cpu')
    params = Params(project_file)
    
    input_sizes = [512, 640, 768, 896, 1024, 1280, 1280, 1536, 1536]
    input_size = input_sizes[compound_coef]
    
    print(f"Creating model with {len(params.obj_list)} classes, compound_coef={compound_coef}")
    
    # Load your model
    backbone = EfficientDetBackbone(
        num_classes=len(params.obj_list), 
        compound_coef=compound_coef, 
        onnx_export=True,
        ratios=eval(params.anchors_ratios), 
        scales=eval(params.anchors_scales)
    ).to(device)
    
    # Fix swish issues specific to your model
    swish_fixed = fix_swish_for_your_model(backbone)
    if not swish_fixed:
        print("⚠ Warning: Swish fixing failed, continuing anyway...")
    
    # Load weights
    print(f"Loading weights from: {weights_path}")
    backbone.load_state_dict(torch.load(weights_path, map_location=device))
    backbone.eval()
    
    # Test inference with fixed test
    print("Testing model inference...")
    inference_ok, outputs = test_model_inference_fixed(backbone, input_size)
    
    if not inference_ok:
        print("❌ Model inference failed, cannot proceed with ONNX conversion")
        return False
    
    # Create ONNX wrapper
    model = YourModelONNXWrapper(backbone, score_threshold, nms_threshold, max_detections)
    model.eval()
    
    # Test wrapper
    dummy_input = torch.randn((1, 3, input_size, input_size), dtype=torch.float32)
    print("Testing ONNX wrapper...")
    
    with torch.no_grad():
        wrapper_outputs = model(dummy_input)
        print(f"✓ Wrapper test successful, output shapes: {[t.shape for t in wrapper_outputs]}")
        
        # Check for actual content
        total_nonzero = sum(torch.count_nonzero(t).item() for t in wrapper_outputs)
        print(f"✓ Total non-zero elements: {total_nonzero}")
        
        if total_nonzero == 0:
            print("⚠ Warning: All outputs are zero, check model weights or threshold")
    
    # Export to ONNX
    try:
        print(f"Converting to ONNX (input_size={input_size})...")
        torch.onnx.export(
            model,
            dummy_input,
            output_path,
            verbose=False,
            input_names=['input'],
            output_names=['boxes', 'scores', 'classes'],
            opset_version=11,
            do_constant_folding=True,
            export_params=True,
            training=torch.onnx.TrainingMode.EVAL,
            dynamic_axes={
                'input': {0: 'batch_size'},
                'boxes': {0: 'batch_size'},
                'scores': {0: 'batch_size'},
                'classes': {0: 'batch_size'}
            }
        )
        
        # Check file size
        file_size = os.path.getsize(output_path) / (1024 * 1024)  # MB
        print(f"✓ ONNX export completed")
        print(f"✓ File size: {file_size:.1f} MB")
        
        if file_size < 10.0:  # Should be 50-100MB for EfficientDet
            print("⚠ Warning: File size seems small for EfficientDet model")
            return False
        
        # Validate
        validate_conversion(model, output_path, dummy_input)
        return True
        
    except Exception as e:
        print(f"❌ ONNX export failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def validate_conversion(pytorch_model, onnx_path, test_input):
    """Validate the ONNX conversion"""
    try:
        import onnxruntime as ort
        print("Validating ONNX conversion...")
        
        # Load ONNX model
        session = ort.InferenceSession(onnx_path, providers=['CPUExecutionProvider'])
        
        # Get PyTorch output
        with torch.no_grad():
            pytorch_outputs = pytorch_model(test_input)
        
        # Get ONNX output
        onnx_input = {session.get_inputs()[0].name: test_input.numpy()}
        onnx_outputs = session.run(None, onnx_input)
        
        print("Validation results:")
        all_good = True
        
        for i, (pt_out, onnx_out) in enumerate(zip(pytorch_outputs, onnx_outputs)):
            pt_np = pt_out.detach().numpy()
            diff = np.abs(pt_np - onnx_out).max()
            
            pt_nonzero = np.count_nonzero(pt_np)
            onnx_nonzero = np.count_nonzero(onnx_out)
            
            print(f"  Output {i}:")
            print(f"    Shape: {pt_out.shape} vs {onnx_out.shape}")
            print(f"    Max diff: {diff:.2e}")
            print(f"    Non-zero: PyTorch={pt_nonzero}, ONNX={onnx_nonzero}")
            
            if diff < 1e-3 and abs(pt_nonzero - onnx_nonzero) < 20:
                print(f"    ✓ Good match")
            else:
                print(f"    ⚠ Potential issue")
                all_good = False
        
        if all_good:
            print("✓ ONNX validation successful")
        else:
            print("⚠ Validation found some issues")
            
        return all_good
        
    except ImportError:
        print("⚠ onnxruntime not available, skipping validation")
        return True
    except Exception as e:
        print(f"❌ Validation failed: {e}")
        return False

if __name__ == '__main__':
    parser = argparse.ArgumentParser('Convert Your Model to ONNX (Targeted Fix)')
    parser.add_argument('-p', '--project', type=str, default='projects/abhil.yml')
    parser.add_argument('-c', '--compound_coef', type=int, default=2)
    parser.add_argument('-w', '--weights', type=str, required=True)
    parser.add_argument('-o', '--output', type=str, default='efficientdet_targeted_fix.onnx')
    parser.add_argument('--score_threshold', type=float, default=0.05)
    parser.add_argument('--nms_threshold', type=float, default=0.5)
    parser.add_argument('--max_detections', type=int, default=100)
    
    args = parser.parse_args()
    
    if not os.path.exists(args.weights):
        print(f"❌ Weights file not found: {args.weights}")
        exit(1)
    
    success = convert_your_model_to_onnx(
        args.project, args.weights, args.output, args.compound_coef,
        args.score_threshold, args.nms_threshold, args.max_detections
    )
    
    if success:
        print(f"✅ Conversion completed successfully: {args.output}")
    else:
        print("❌ Conversion failed")

# Add to your train.py - Ray Data integration

import ray.data
from ray.data.preprocessors import Batch
import pyarrow as pa

class RayCocoDataset:
    def __init__(self, data_path, set_name, transform=None, obj_list=None):
        self.data_path = data_path
        self.set_name = set_name
        self.transform = transform
        self.obj_list = obj_list
        
        # Create Ray dataset
        self.ray_dataset = self._create_ray_dataset()
    
    def _create_ray_dataset(self):
        """Create Ray dataset from COCO data"""
        import json
        from PIL import Image
        import numpy as np
        
        # Load COCO annotations
        ann_file = f"{self.data_path}/annotations/instances_{self.set_name}.json"
        with open(ann_file, 'r') as f:
            coco_data = json.load(f)
        
        # Create dataset records
        records = []
        for img in coco_data['images']:
            img_path = f"{self.data_path}/{self.set_name}/{img['file_name']}"
            
            # Get annotations for this image
            img_anns = [ann for ann in coco_data['annotations'] 
                       if ann['image_id'] == img['id']]
            
            record = {
                'image_path': img_path,
                'image_id': img['id'],
                'width': img['width'],
                'height': img['height'],
                'annotations': img_anns
            }
            records.append(record)
        
        # Create Ray dataset
        return ray.data.from_items(records)
    
    def get_dataloader(self, batch_size, shuffle=True):
        """Get Ray dataset iterator"""
        if shuffle:
            dataset = self.ray_dataset.random_shuffle()
        else:
            dataset = self.ray_dataset
            
        return dataset.iter_batches(
            batch_size=batch_size,
            batch_format="pandas"
        )

# Modified train function for Ray Data
def train_func_with_ray_data(config):
    """Training function using Ray Data"""
    
    # ... (previous setup code remains same)
    
    # Create Ray datasets instead of regular datasets
    training_ray_dataset = RayCocoDataset(
        data_path=os.path.join(config['data_path'], params.project_name),
        set_name=params.train_set,
        transform=transforms.Compose([
            Normalizer(mean=params.mean, std=params.std),
            Augmenter(),
            Resizer(input_sizes[config['compound_coef']])
        ]),
        obj_list=params.obj_list
    )
    
    val_ray_dataset = RayCocoDataset(
        data_path=os.path.join(config['data_path'], params.project_name),
        set_name=params.val_set,
        transform=transforms.Compose([
            Normalizer(mean=params.mean, std=params.std),
            Resizer(input_sizes[config['compound_coef']])
        ]),
        obj_list=params.obj_list
    )
    
    # Get data iterators
    train_iterator = training_ray_dataset.get_dataloader(
        batch_size=config['batch_size'], 
        shuffle=True
    )
    
    val_iterator = val_ray_dataset.get_dataloader(
        batch_size=config['batch_size'], 
        shuffle=False
    )
    
    # ... (rest of training code with iterators)

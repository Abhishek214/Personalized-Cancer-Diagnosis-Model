# Debug Script for ONNX Conversion Issues
# File: debug_conversion.py

import torch
import yaml
import os
from backbone import EfficientDetBackbone

class Params:
    def __init__(self, project_file):
        self.params = yaml.safe_load(open(project_file).read())
    def __getattr__(self, item):
        return self.params.get(item, None)

def debug_model_structure(model, prefix=""):
    """Debug model structure to find swish-related issues"""
    print(f"\n=== Model Structure Debug ===")
    
    def print_module_info(module, name="", level=0):
        indent = "  " * level
        module_type = type(module).__name__
        print(f"{indent}{name}: {module_type}")
        
        # Check for swish-related attributes
        if hasattr(module, 'swish'):
            print(f"{indent}  ✓ Has swish attribute: {type(module.swish)}")
        if hasattr(module, '_swish'):
            print(f"{indent}  ✓ Has _swish attribute: {type(module._swish)}")
        if hasattr(module, 'set_swish'):
            print(f"{indent}  ✓ Has set_swish method")
        
        # Recursively check children (limit depth to avoid too much output)
        if level < 3:
            for child_name, child_module in module.named_children():
                print_module_info(child_module, child_name, level + 1)
    
    print_module_info(model)

def test_model_inference(model, input_size=768):
    """Test if model can run inference properly"""
    print(f"\n=== Model Inference Test ===")
    
    try:
        model.eval()
        dummy_input = torch.randn(1, 3, input_size, input_size)
        
        print(f"Input shape: {dummy_input.shape}")
        
        with torch.no_grad():
            outputs = model(dummy_input)
            
        print(f"✓ Inference successful!")
        print(f"Number of outputs: {len(outputs)}")
        
        for i, output in enumerate(outputs):
            print(f"  Output {i}: {output.shape}, dtype: {output.dtype}")
            non_zero = torch.count_nonzero(output)
            print(f"    Non-zero elements: {non_zero}")
            
        return True, outputs
        
    except Exception as e:
        print(f"❌ Inference failed: {e}")
        return False, None

def check_swish_implementations(model):
    """Check and fix swish implementations"""
    print(f"\n=== Swish Implementation Check ===")
    
    swish_modules = []
    
    def find_swish_modules(module, path=""):
        for name, child in module.named_children():
            child_path = f"{path}.{name}" if path else name
            
            # Check if this is a swish-like module
            if 'swish' in name.lower() or 'Swish' in str(type(child)):
                swish_modules.append((child_path, child))
                print(f"Found swish module: {child_path} -> {type(child)}")
            
            find_swish_modules(child, child_path)
    
    find_swish_modules(model)
    
    print(f"Total swish modules found: {len(swish_modules)}")
    
    # Try to fix swish modules
    fixed_count = 0
    for path, module in swish_modules:
        try:
            if hasattr(module, 'set_swish'):
                module.set_swish(memory_efficient=False)
                print(f"✓ Fixed swish at {path}")
                fixed_count += 1
        except Exception as e:
            print(f"⚠ Could not fix swish at {path}: {e}")
    
    print(f"Fixed {fixed_count} swish modules")
    
    return fixed_count > 0

def minimal_onnx_test(model, input_size=768):
    """Test minimal ONNX export without postprocessing"""
    print(f"\n=== Minimal ONNX Export Test ===")
    
    try:
        # Create a wrapper that only does backbone inference
        class MinimalEfficientDet(torch.nn.Module):
            def __init__(self, backbone):
                super().__init__()
                self.backbone = backbone
                
            def forward(self, x):
                features, regression, classification, anchors = self.backbone(x)
                return regression, classification  # Return raw outputs only
        
        minimal_model = MinimalEfficientDet(model)
        minimal_model.eval()
        
        dummy_input = torch.randn(1, 3, input_size, input_size)
        
        # Test inference
        with torch.no_grad():
            outputs = minimal_model(dummy_input)
            print(f"✓ Minimal model inference successful")
            print(f"Outputs: {[o.shape for o in outputs]}")
        
        # Try ONNX export
        output_path = "test_minimal.onnx"
        torch.onnx.export(
            minimal_model,
            dummy_input,
            output_path,
            verbose=False,
            input_names=['input'],
            output_names=['regression', 'classification'],
            opset_version=11
        )
        
        file_size = os.path.getsize(output_path) / (1024 * 1024)
        print(f"✓ Minimal ONNX export successful")
        print(f"File size: {file_size:.1f} MB")
        
        # Clean up
        if os.path.exists(output_path):
            os.remove(output_path)
            
        return True
        
    except Exception as e:
        print(f"❌ Minimal ONNX export failed: {e}")
        return False

def main():
    import argparse
    
    parser = argparse.ArgumentParser('Debug ONNX Conversion')
    parser.add_argument('-p', '--project', type=str, default='projects/abhil.yml')
    parser.add_argument('-c', '--compound_coef', type=int, default=2)
    parser.add_argument('-w', '--weights', type=str, required=True)
    
    args = parser.parse_args()
    
    print("=== EfficientDet ONNX Conversion Debug ===")
    print(f"Project: {args.project}")
    print(f"Compound coef: {args.compound_coef}")
    print(f"Weights: {args.weights}")
    
    # Load parameters
    params = Params(args.project)
    print(f"Classes: {len(params.obj_list)}")
    
    # Create model
    print(f"\nCreating model...")
    model = EfficientDetBackbone(
        num_classes=len(params.obj_list), 
        compound_coef=args.compound_coef, 
        onnx_export=True,
        ratios=eval(params.anchors_ratios), 
        scales=eval(params.anchors_scales)
    )
    
    # Load weights
    print(f"Loading weights...")
    model.load_state_dict(torch.load(args.weights, map_location='cpu'))
    
    # Run debug tests
    debug_model_structure(model)
    
    swish_fixed = check_swish_implementations(model)
    
    inference_ok, outputs = test_model_inference(model)
    
    if inference_ok:
        minimal_export_ok = minimal_onnx_test(model)
    else:
        minimal_export_ok = False
    
    # Summary
    print(f"\n=== Debug Summary ===")
    print(f"Swish modules fixed: {'✓' if swish_fixed else '❌'}")
    print(f"Model inference: {'✓' if inference_ok else '❌'}")
    print(f"Minimal ONNX export: {'✓' if minimal_export_ok else '❌'}")
    
    if inference_ok and minimal_export_ok:
        print(f"\n✅ Model looks good for ONNX conversion!")
        print(f"Try running the robust conversion script.")
    else:
        print(f"\n❌ Model has issues that need to be fixed first.")
        
        if not inference_ok:
            print("  - Fix model inference issues first")
        if not minimal_export_ok:
            print("  - ONNX export compatibility issues detected")

if __name__ == '__main__':
    main()

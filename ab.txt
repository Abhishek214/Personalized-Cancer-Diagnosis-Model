# Author: Zylo117

"""
COCO-Style Evaluations

Put images here: datasets/your_project_name/val_set_name/*.jpg  
Put annotations here: datasets/your_project_name/annotations/instances_{val_set_name}.json  
Put weights here: /path/to/your/weights/*.pth  
Change compound_coef
"""

import json
import os
import argparse
import torch
import yaml
import numpy as np
from tqdm import tqdm

from pycocotools.coco import COCO
from pycocotools.cocoeval import COCOeval

from backbone import EfficientDetBackbone
from efficientdet.utils import BBoxTransform, ClipBoxes
from utils.utils import preprocess, invert_affine, postprocess, boolean_string

ap = argparse.ArgumentParser()
ap.add_argument('-p', '--project', type=str, default='coco', help='Project file that contains parameters')
ap.add_argument('-c', '--compound_coef', type=int, default=0, help='Coefficients of efficientdet')
ap.add_argument('-w', '--weights', type=str, default=None, help='/path/to/weights')
ap.add_argument('--nms_threshold', type=float, default=0.5, help='NMS threshold, don\'t change it for testing purposes')
ap.add_argument('--cuda', type=boolean_string, default=False)
ap.add_argument('--device', type=int, default=0)
ap.add_argument('--float16', type=boolean_string, default=False)
ap.add_argument('--override', type=boolean_string, default=True, help='Override previous bbox results file if exists')
args = ap.parse_args()

compound_coef = args.compound_coef
nms_threshold = args.nms_threshold
use_cuda = args.cuda
gpu = args.device
use_float16 = args.float16
override_prev_results = args.override
project_name = args.project
weights_path = f'weights/efficientdet-d{compound_coef}.pth' if args.weights is None else args.weights

print(f'Running coco-style evaluation on project {project_name}, weights {weights_path}...')

params = yaml.safe_load(open(f'projects/{project_name}.yml'))
obj_list = params['obj_list']

input_sizes = [512, 640, 768, 896, 1024, 1280, 1280, 1536, 1536]

def evaluate_coco(img_path, set_name, image_ids, coco, model, threshold=0.05):
    results = []

    regressBoxes = BBoxTransform()
    clipBoxes = ClipBoxes()

    for image_id in tqdm(image_ids):
        image_info = coco.loadImgs(image_id)[0]
        image_path = img_path + image_info['file_name']

        ori_imgs, framed_imgs, framed_metas = preprocess(
            image_path,
            max_size=input_sizes[compound_coef],
            mean=params['mean'],
            std=params['std']
        )

        x = torch.from_numpy(framed_imgs[0])

        if use_cuda:
            x = x.cuda(gpu)
            if use_float16:
                x = x.half()
            else:
                x = x.float()
        else:
            x = x.float()

        x = x.unsqueeze(0).permute(0, 3, 1, 2)

        features, regression, classification, anchors = model(x)

        preds = postprocess(
            x,
            anchors, regression, classification,
            regressBoxes, clipBoxes,
            threshold, nms_threshold
        )

        if not preds:
            continue

        preds = invert_affine(framed_metas, preds)[0]

        scores = preds['scores']
        class_ids = preds['class_ids']
        rois = preds['rois']

        if rois.shape[0] > 0:
            # convert [x1,y1,x2,y2] to [x1,y1,w,h]
            rois[:, 2] -= rois[:, 0]
            rois[:, 3] -= rois[:, 1]

            bbox_score = scores

            for roi_id in range(rois.shape[0]):
                score = float(bbox_score[roi_id])
                label = int(class_ids[roi_id])
                box = rois[roi_id, :]

                image_result = {
                    'image_id': image_id,
                    'category_id': label + 1,
                    'score': float(score),
                    'bbox': box.tolist(),
                }

                results.append(image_result)

    if not len(results):
        raise Exception("The model does not provide any valid output, check model architecture and the data input")

    # Write output
    filepath = f'{set_name}_bbox_results.json'
    if os.path.exists(filepath):
        os.remove(filepath)
    json.dump(results, open(filepath, 'w'), indent=4)

def _eval(coco_gt, image_ids, pred_json_path):
    # Load results in COCO evaluation tool
    coco_pred = coco_gt.loadRes(pred_json_path)

    # Run COCO evaluation
    print('BBox')
    coco_eval = COCOeval(coco_gt, coco_pred, 'bbox')
    coco_eval.params.imgIds = image_ids
    coco_eval.evaluate()
    coco_eval.accumulate()
    coco_eval.summarize()
    
    # Get per-class metrics
    print("\n" + "="*50)
    print("PER-CLASS METRICS")
    print("="*50)
    
    # Get category information
    cats = coco_gt.loadCats(coco_gt.getCatIds())
    cat_names = [cat['name'] for cat in cats]
    
    # Calculate per-class AP at IoU=0.50:0.95
    precisions = coco_eval.eval['precision']
    # precision has dims (iou, recall, cls, area range, max dets)
    # We want to average over IoU thresholds (0.5:0.95) and recall values
    
    print(f"{'Class':<15} {'AP@0.5:0.95':<12} {'AP@0.5':<10} {'AP@0.75':<10}")
    print("-" * 50)
    
    class_aps = []
    class_ap50s = []
    class_ap75s = []
    
    for cls_idx, class_name in enumerate(cat_names):
        # AP at IoU=0.50:0.95 (average over all IoU thresholds)
        cls_precision = precisions[:, :, cls_idx, 0, 2]  # area=all, maxDets=100
        cls_precision = cls_precision[cls_precision > -1]  # Remove -1 values
        
        if len(cls_precision) > 0:
            ap_all = np.mean(cls_precision)
        else:
            ap_all = 0.0
            
        # AP at IoU=0.5
        cls_precision_50 = precisions[0, :, cls_idx, 0, 2]  # IoU=0.5
        cls_precision_50 = cls_precision_50[cls_precision_50 > -1]
        
        if len(cls_precision_50) > 0:
            ap50 = np.mean(cls_precision_50)
        else:
            ap50 = 0.0
            
        # AP at IoU=0.75  
        cls_precision_75 = precisions[5, :, cls_idx, 0, 2]  # IoU=0.75 (index 5)
        cls_precision_75 = cls_precision_75[cls_precision_75 > -1]
        
        if len(cls_precision_75) > 0:
            ap75 = np.mean(cls_precision_75)
        else:
            ap75 = 0.0
        
        class_aps.append(ap_all)
        class_ap50s.append(ap50)
        class_ap75s.append(ap75)
        
        print(f"{class_name:<15} {ap_all:<12.4f} {ap50:<10.4f} {ap75:<10.4f}")
    
    # Print mean values
    print("-" * 50)
    print(f"{'Mean':<15} {np.mean(class_aps):<12.4f} {np.mean(class_ap50s):<10.4f} {np.mean(class_ap75s):<10.4f}")
    
    # Additional detailed metrics per class
    print("\n" + "="*70)
    print("DETAILED PER-CLASS METRICS")
    print("="*70)
    print(f"{'Class':<15} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'#GT':<8}")
    print("-" * 70)
    
    for cls_idx, class_name in enumerate(cat_names):
        # Get ground truth and detection counts for this class
        gt_ids = coco_gt.getAnnIds(imgIds=image_ids, catIds=[cats[cls_idx]['id']])
        gt_count = len(gt_ids)
        
        # Get detections for this class from predictions
        pred_ids = coco_pred.getAnnIds(imgIds=image_ids, catIds=[cats[cls_idx]['id']])
        pred_count = len(pred_ids)
        
        # Calculate precision and recall at IoU=0.5
        if gt_count > 0:
            # Use the evaluation results to get TP, FP counts
            cls_precision_50 = precisions[0, :, cls_idx, 0, 2]  # IoU=0.5
            valid_precision = cls_precision_50[cls_precision_50 > -1]
            
            if len(valid_precision) > 0:
                precision = np.mean(valid_precision)
                # Approximate recall calculation
                recall = min(1.0, pred_count / gt_count * precision)
            else:
                precision = 0.0
                recall = 0.0
                
            # Calculate F1 score
            if precision + recall > 0:
                f1 = 2 * (precision * recall) / (precision + recall)
            else:
                f1 = 0.0
        else:
            precision = recall = f1 = 0.0
        
        print(f"{class_name:<15} {precision:<12.4f} {recall:<12.4f} {f1:<12.4f} {gt_count:<8d}")

if __name__ == '__main__':
    SET_NAME = params['val_set']
    VAL_GT = f'datasets/{params["project_name"]}/annotations/instances_{SET_NAME}.json'
    VAL_IMGS = f'datasets/{params["project_name"]}/{SET_NAME}/'
    MAX_IMAGES = 100000

    coco_gt = COCO(VAL_GT)
    image_ids = coco_gt.getImgIds()[:MAX_IMAGES]

    if override_prev_results or not os.path.exists(f'{SET_NAME}_bbox_results.json'):
        model = EfficientDetBackbone(
            compound_coef=compound_coef,
            num_classes=len(obj_list),
            ratios=eval(params['anchors_ratios']),
            scales=eval(params['anchors_scales'])
        )
        model.load_state_dict(torch.load(weights_path, map_location=torch.device('cpu')))
        model.requires_grad_(False)
        model.eval()

        if use_cuda:
            model.cuda(gpu)
            if use_float16:
                model.half()

        evaluate_coco(VAL_IMGS, SET_NAME, image_ids, coco_gt, model)

    _eval(coco_gt, image_ids, f'{SET_NAME}_bbox_results.json')

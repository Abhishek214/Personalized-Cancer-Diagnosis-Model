import numpy as np
import random
import os
import cv2
from PIL import Image
from pathlib import Path
import albumentations as A

class ChopOverlay:
    def __init__(self, chop_image_path: str):
        """Initialize with path to chop image"""
        self.chop_image = Image.open(chop_image_path).convert("RGBA")

    def remove_background(self, threshold: int = 180) -> Image.Image:
        """Remove background from chop image using advanced masking"""
        img = self.chop_image.copy().convert('RGBA')
        data = np.array(img)
        
        # Convert to grayscale to find text
        gray = np.mean(data[:, :, :3], axis=2)
        
        # Create mask for dark text (keep) vs light background (remove)
        text_mask = gray < threshold
        
        # Set alpha channel - only keep dark text areas
        data[:, :, 3] = np.where(text_mask, 255, 0)
        
        return Image.fromarray(data, 'RGBA')

    def match_background_color(self, document: Image.Image, position: tuple[int, int], 
                             sample_radius: int = 20) -> Image.Image:
        """Match chop background to document background color at position"""
        chop = self.remove_background()
        
        # Sample background color from document
        x, y = position
        crop_area = (max(0, x-sample_radius), max(0, y-sample_radius),
                    min(document.width, x+sample_radius), 
                    min(document.height, y+sample_radius))
        
        sample_area = document.crop(crop_area)
        avg_color = tuple(np.mean(np.array(sample_area), axis=(0,1)).astype(int))
        
        # Ensure we have RGB values only, convert to int
        if len(avg_color) >= 3:
            bg_color = (int(avg_color[0]), int(avg_color[1]), int(avg_color[2]), 255)
        else:
            bg_color = (255, 255, 255, 255)  # fallback to white
        
        # Create background layer with sampled color
        bg_layer = Image.new('RGBA', chop.size, bg_color)
        
        # Composite chop over background
        return Image.alpha_composite(bg_layer, chop)

    def blend_with_document(self, document_path: str, position: tuple[int, int], 
                           opacity: float = 0.8, size: tuple[int, int] = None,
                           output_path: str = "output_document.png") -> Image.Image:
        """Blend chop with document at specified position"""
        # Load document
        document = Image.open(document_path).convert("RGBA")
        
        # Process chop - remove background first
        chop = self.remove_background()
        
        # Resize chop if specified
        if size:
            chop = chop.resize(size, Image.Resampling.LANCZOS)
        
        # Adjust opacity
        if opacity < 1.0:
            # Apply opacity to alpha channel only
            chop_data = np.array(chop)
            chop_data[:, :, 3] = (chop_data[:, :, 3] * opacity).astype(np.uint8)
            chop = Image.fromarray(chop_data, 'RGBA')
        
        # Paste chop directly onto document using alpha channel for transparency
        result = document.copy()
        result.paste(chop, position, chop)  # Use chop as mask for transparency
        
        # Save result
        result.save(output_path, "PNG")
        return result

def apply_chop_transformations(chop_image):
    """Apply realistic transformations to chop images"""
    transform = A.Compose([
        A.Rotate(limit=15, p=0.8, border_mode=cv2.BORDER_CONSTANT, value=255),
        A.VerticalFlip(p=0.5),
        A.RandomScale(scale_limit=0.3, p=0.7),
        A.GaussianBlur(blur_limit=(1, 3), p=0.3),
        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),
        A.GaussNoise(var_limit=(10, 50), p=0.2),
    ])
    return transform(image=chop_image)['image']

def find_safe_areas(document):
    """Find areas where chops can be placed - improved version"""
    height, width = document.shape[:2]
    gray = cv2.cvtColor(document, cv2.COLOR_BGR2GRAY)
    
    # Use adaptive thresholding to better detect text and content
    binary = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)
    
    # Invert so text/content areas are white
    binary = cv2.bitwise_not(binary)
    
    # More aggressive morphological operations to expand text/content areas
    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (15, 15))
    occupied_mask = cv2.dilate(binary, kernel, iterations=3)
    
    # Create free mask (areas without text/content)
    free_mask = cv2.bitwise_not(occupied_mask)
    
    # Further erode free areas to ensure we're well away from content
    erode_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (20, 20))
    free_mask = cv2.erode(free_mask, erode_kernel, iterations=2)
    
    margin = 100
    safe_areas = []
    grid_size = 150
    
    for y in range(margin, height - margin, grid_size):
        for x in range(margin, width - margin, grid_size):
            roi = free_mask[y:y+grid_size, x:x+grid_size]
            if roi.size > 0:
                if np.sum(roi) / (roi.size * 255) > 0.8:
                    safe_areas.append((x, y, x+grid_size, y+grid_size))
    
    return safe_areas

def place_chop_on_document(document_path, chop_path, position, scale_factor=1.0):
    """Place chop on document using ChopOverlay class"""
    # Load chop using ChopOverlay
    chop_overlay = ChopOverlay(chop_path)
    
    # Load document as PIL Image for processing
    document_pil = Image.open(document_path).convert("RGBA")
    
    # Calculate size if scaling
    original_size = chop_overlay.chop_image.size
    if scale_factor != 1.0:
        new_size = (int(original_size[0] * scale_factor), int(original_size[1] * scale_factor))
    else:
        new_size = None
    
    # Apply transformations to chop first
    chop_array = np.array(chop_overlay.chop_image.convert("RGB"))
    chop_transformed = apply_chop_transformations(chop_array)
    
    # Save transformed chop temporarily
    temp_chop_path = "temp_chop.png"
    Image.fromarray(chop_transformed).save(temp_chop_path)
    
    # Create new ChopOverlay with transformed image
    chop_overlay_transformed = ChopOverlay(temp_chop_path)
    
    # Blend with document
    result = chop_overlay_transformed.blend_with_document(
        document_path, position, opacity=1.0, size=new_size, output_path="temp_result.png"
    )
    
    # Clean up temp file
    os.remove(temp_chop_path)
    
    # Convert back to OpenCV format
    result_cv = cv2.cvtColor(np.array(result.convert("RGB")), cv2.COLOR_RGB2BGR)
    
    return result_cv

def augment_images(chop_images_dir, document_images_dir, output_dir, num_chops_per_doc=3):
    """Main function to augment images with labelme annotations"""
    import json
    
    chop_dir = Path(chop_images_dir)
    doc_dir = Path(document_images_dir)
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    
    # Load image paths
    chop_images = list(chop_dir.glob("*.jpg")) + list(chop_dir.glob("*.png"))
    doc_images = list(doc_dir.glob("*.jpg")) + list(doc_dir.glob("*.png"))
    
    for doc_path in doc_images:
        # Load corresponding JSON annotation file
        json_path = doc_path.with_suffix('.json')
        if not json_path.exists():
            print(f"Warning: No annotation file found for {doc_path}")
            continue
            
        # Load annotation data
        with open(json_path, 'r') as f:
            annotation_data = json.load(f)
        
        document = cv2.imread(str(doc_path))
        if document is None:
            continue
            
        safe_areas = find_safe_areas(document)
        num_chops = min(num_chops_per_doc, len(safe_areas))
        
        if num_chops == 0:
            continue
            
        selected_areas = random.sample(safe_areas, num_chops)
        
        # Start with original document for each iteration
        current_doc_path = str(doc_path)
        chop_annotations = []  # Store chop placement info
        
        for i in range(num_chops):
            chop_path = random.choice(chop_images)
            
            scale_factor = random.uniform(0.3, 1.2)
            x1, y1, x2, y2 = selected_areas[i]
            
            # Calculate chop size for position bounds
            chop_img = Image.open(str(chop_path))
            chop_w, chop_h = chop_img.size
            scaled_w = int(chop_w * scale_factor)
            scaled_h = int(chop_h * scale_factor)
            
            pos_x = random.randint(x1, max(x1, x2 - scaled_w))
            pos_y = random.randint(y1, max(y1, y2 - scaled_h))
            
            # Store chop annotation info
            chop_annotation = {
                "label": "chop",
                "points": [
                    [pos_x, pos_y],  # top-left
                    [pos_x + scaled_w, pos_y + scaled_h]  # bottom-right
                ],
                "group_id": None,
                "shape_type": "rectangle",
                "flags": {}
            }
            chop_annotations.append(chop_annotation)
            
            # Place chop and get result
            result_document = place_chop_on_document(
                current_doc_path, str(chop_path), (pos_x, pos_y), scale_factor
            )
            
            # Save intermediate result for next iteration
            temp_doc_path = f"temp_doc_{i}.png"
            cv2.imwrite(temp_doc_path, result_document)
            current_doc_path = temp_doc_path
        
        # Generate output filenames
        random_suffix = random.randint(1000, 9999)
        output_name = f"{doc_path.stem}_with_chops_{random_suffix}"
        output_image_file = output_path / f"{output_name}.jpg"
        output_json_file = output_path / f"{output_name}.json"
        
        # Save final augmented image
        cv2.imwrite(str(output_image_file), result_document)
        
        # Update annotation data
        updated_annotation = annotation_data.copy()
        updated_annotation["imagePath"] = f"{output_name}.jpg"
        
        # Add chop annotations to existing shapes
        if "shapes" not in updated_annotation:
            updated_annotation["shapes"] = []
        updated_annotation["shapes"].extend(chop_annotations)
        
        # Update image dimensions if needed
        h, w = result_document.shape[:2]
        updated_annotation["imageHeight"] = h
        updated_annotation["imageWidth"] = w
        
        # Save updated annotation file
        with open(output_json_file, 'w') as f:
            json.dump(updated_annotation, f, indent=2)
        
        print(f"Saved: {output_image_file} and {output_json_file}")
        
        # Clean up temporary files
        for i in range(num_chops):
            temp_file = f"temp_doc_{i}.png"
            if os.path.exists(temp_file):
                os.remove(temp_file)
        
        # Clean up temp result if exists
        if os.path.exists("temp_result.png"):
            os.remove("temp_result.png")

# Usage
if __name__ == "__main__":
    augment_images(
        chop_images_dir="./chop_images",
        document_images_dir="./document_images", 
        output_dir="./output",
        num_chops_per_doc=3
    )
